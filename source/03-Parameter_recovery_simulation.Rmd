%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
```

# Simulation: Comparing two estimators and how effectively they recover population parameters

Item factor analysis (IFA) is a common method for determine the effectiveness and structure of items in
psychological tests. Two approaches currently exist: full-information IFA via method from the item response
theory framework, and limited-information IFA from the structural equation modeling framework. The question is,
under which circumstances should either method be used? Here we explore a simple one factor test structure
to determine which approach can recover the item parameters when varying test length and sample size.

Unforunatly, the `mirt` package uses a slightly different parameterization of the IFA problem, implementing 
a logistic model rather than a normal ogive model. Historically, the response curves have been equated 
by applying a scaling correction of $D = 1.702$ to help fix this issue, which as can be seen below makes the 
response curves very similar (as it turns out, logistic models have slightly thicker tails than ogive models).

To give the following simulation the benifit of the doubt, the data will be 
generated from the normal ogive model (implying that `lavaan` is fitting the correct model) while `mirt`
is only fitting an approximation to this model by translating the parameters. From this we will see whether
FIML with a logistic model will better approximate the normal ogive parmaters compared to the limited information
DWLS approach available in `lavaan`.

```{r}
P_logit <- function(a, d, Theta) exp(a * Theta + d) / (1 + exp(a * Theta + d))
P_ogive <- function(a, d, Theta) pnorm(a * Theta + d)

Theta <- seq(-5,5,length.out=200)
a <- 0.5
d <- -.5
D <- 1.702

example <- data.frame(Theta, logit=P_logit(a*D, d*D, Theta), ogive=P_ogive(a, d, Theta))
plot(Theta, example$logit, type = 'l', ylab = 'P', las=1)
lines(Theta, example$ogive, col = 'red')
```


### Define the conditions

Start by defining the conditions to be studied. Because we are interested in a number of slopes and intercepts
it is often convenient to save long strings of numbers to a single object to be passed to `fixed_design_elements`.
Naturally, these could be included in the simulation source code directly, but often will take up a large amount
of space (could be hundreds of lines), therefore passing a list object to `runSimulation()` may be more 
convinient. 

```{r}
sample_sizes <- c(250, 500, 1000)
Design <- data.frame(sample_size=sample_sizes)

# save extra verbose information in an external file, and read it in later
set.seed(1)
pars_10 <- rbind(a = round(rlnorm(10, .3, .5)/1.702, 2), 
                          d = round(rnorm(10, 0, .5)/1.702, 2))
pars_10
pars_30 <- rbind(a = round(rlnorm(30, .3, .5)/1.702, 2), 
                          d = round(rnorm(30, 0, .5)/1.702, 2))
pars_30
```

The above slopes, when standardized, give the following factor loadings and communalities: 

```{r}
a10 <- pars_10['a', ]; a30 <- pars_30['a', ]
load10 <- a10 / sqrt(1 + a10^2)
load30 <- a30 / sqrt(1 + a30^2)
rbind(F=load10, h2=load10^2)
rbind(F=load30, h2=load30^2)
```

### Define the functions

As usual, define the functions of interest. Here we make sure that `lavaan` and `mirt` are loaded in when they
are required by calling a suitable `library()` call to make their functions available (required when 
running simulations in parallel). Here we only collect information on the slope parameters, mainly because they
are the most interesting to study anyway (intercepts become more important in IRT methods).

```{r}
library(SimDesign)
# SimDesign_functions()
```

```{r}
Generate <- function(condition, fixed_design_elements = NULL) {
    
    P_ogive <- function(a, d, Theta) pnorm(a*Theta + d)
    
    N <- condition$sample_size
    nitems <- ncol(fixed_design_elements)
        
    #extract objects from fixed_design_elements
    a <- fixed_design_elements['a', ]
    d <- fixed_design_elements['d', ]
    
    dat <- matrix(NA, N, nitems)
    colnames(dat) <- paste0('item_', 1:nitems)
    Theta <- rnorm(N)
    for(j in 1:nitems){
        p <- P_ogive(a[j], d[j], Theta)
        for(i in 1:N)
            dat[i,j] <- sample(c(1,0), 1, prob = c(p[i], 1 - p[i])) 
    }
    return(as.data.frame(dat))
}

Analyse <- function(condition, dat, fixed_design_elements = NULL, parameters = NULL) {
    
    library(mirt)
    library(lavaan)
    nitems <- ncol(dat)
    
    # (optional) could use better starting values from fixed_design_elements here too
    mod <- try(mirt(dat, 1L, verbose=FALSE), silent=TRUE)
    check_error(mod)
    if(!mod@converge) stop('did not converge')
    cfs <- coef(mod, simplify = TRUE, digits = Inf)
    FIML_as <- cfs$items[,1L] / 1.702
    
    lavmod <- paste0('F =~ ', paste0('NA*', colnames(dat)[1L], ' + '), 
                     paste0(colnames(dat)[-1L], collapse = ' + '),
                     '\nF ~~ 1*F')
    lmod <- try(sem(lavmod, dat, ordered = colnames(dat)))
    check_error(lmod)
    if(!lmod@Fit@converged) stop('did not converge')
    cfs2 <- coef(lmod) 
    DWLS_alpha <- cfs2[1L:nitems]
    const <- sqrt(1 - DWLS_alpha^2) 
    DWLS_as <- DWLS_alpha / const

    return(c(FIML_as=unname(FIML_as), DWLS_as=unname(DWLS_as)))
}

Summarise <- function(condition, results, fixed_design_elements = NULL, parameters_list = NULL) {
    
    pop_as <- fixed_design_elements['a', ]
    pop <- c(pop_as, pop_as)
    
    index <- 1:ncol(results)
    
    obt_bias <- sapply(index, function(ind, obs, pop) bias(obs[,ind], pop[ind]),
                       obs = results, pop = pop)
    obt_RMSE <- sapply(index, function(ind, obs, pop) RMSE(obs[,ind], pop[ind]),
                       obs = results, pop = pop)
    names(obt_bias) <- names(obt_RMSE) <- colnames(results)
    
    ret <- c(bias=obt_bias, RMSE=obt_RMSE)
    return(ret)
}
```

### Run the simulation

Because this simulation takes considerably longer it is recommended to pass the `save = TRUE` to temporarily
save results in case of power outages. Results can be continued by running the identical simulation code as the
initial run, and the function will automatically detect whether any temp files are available and resume the 
simulation at the previously saved location.

```{r cache=TRUE}
res10 <- runSimulation(Design, replications = 200, verbose = FALSE, save = TRUE,
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE,
                       fixed_design_elements=pars_10)
```

```{r cache=TRUE}
res30 <- runSimulation(Design, replications = 200, verbose = FALSE, save = TRUE,
                       generate=Generate, analyse=Analyse, summarise=Summarise, parallel = TRUE,
                       fixed_design_elements=pars_30)
```

### Analyze the results

Sometimes, reshaping and indexing your output can be very helpful. Here we break the analysis into two parts,
though other strategies are certainly possible. Because analyzing simulations is a lot like analyzing empirical
data no one strategy may be the best; you have to use judgment.

#### Ten items

```{r}
# bias in slopes
names10 <- colnames(res10)
bias_as_fiml <- t(res10[,grepl('bias\\.', names10) & grepl('\\_as', names10) & 
                       grepl('FIML', names10)])
colnames(bias_as_fiml) <- sample_sizes
rownames(bias_as_fiml) <- pars_10['a', ]

bias_as_dwls <- t(res10[,grepl('bias\\.', names10) & grepl('\\_as', names10) & 
                       grepl('DWLS', names10)])
colnames(bias_as_dwls) <- sample_sizes
rownames(bias_as_dwls) <- pars_10['a', ]

(out <- list(FIML=bias_as_fiml, DWLS=bias_as_dwls))
sapply(out, colMeans)

# RMSE in slopes
RMSE_as_fiml <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_as', names10) & 
                       grepl('FIML', names10)])
colnames(RMSE_as_fiml) <- sample_sizes
rownames(RMSE_as_fiml) <- pars_10['a', ]

RMSE_as_dwls <- t(res10[,grepl('RMSE\\.', names10) & grepl('\\_as', names10) & 
                       grepl('DWLS', names10)])
colnames(RMSE_as_dwls) <- sample_sizes
rownames(RMSE_as_dwls) <- pars_10['a', ]

(out <- list(FIML=RMSE_as_fiml, DWLS=RMSE_as_dwls))
sapply(out, colMeans)
```

The methods appeared to recover the slope parameters fairly well, became progressively less biased as 
the sample size incread, and parameters were generally recovered with greater efficiency as $N$ increased 
as well. Additionally, there appeared to be an effect relating to the size of the parameters. For IRT parameters,
very large slopes appeared to be recovered with greater bias compared to the DWLS, though both estimators 
had more difficulty recovering the more extreme slopes (see the RMSE plot below).

```{r}
library(ggplot2)
plt <- data.frame(pars = c(pars_10['a', ], pars_10['a', ]), 
                  RMSE = c(RMSE_as_fiml[,'1000'], RMSE_as_dwls[,'1000']),
                  bias = c(bias_as_fiml[,'1000'], bias_as_dwls[,'1000']),
                  estimator = rep(c('FIML', 'DWLS'), each = 10))
ggplot(plt, aes(pars, bias, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by bias for FIML and DWLS estimators')
ggplot(plt, aes(pars, RMSE, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by RMSE for FIML and DWLS estimators')

```

#### Thirty items

```{r}
# bias in slopes
names30 <- colnames(res30)
bias_as_fiml <- t(res30[,grepl('bias\\.', names30) & grepl('\\_as', names30) & 
                       grepl('FIML', names30)])
colnames(bias_as_fiml) <- sample_sizes
rownames(bias_as_fiml) <- pars_30['a', ]

bias_as_dwls <- t(res30[,grepl('bias\\.', names30) & grepl('\\_as', names30) & 
                       grepl('DWLS', names30)])
colnames(bias_as_dwls) <- sample_sizes
rownames(bias_as_dwls) <- pars_30['a', ]

(out <- list(FIML=bias_as_fiml, DWLS=bias_as_dwls))
sapply(out, colMeans)

# RMSE in slopes
RMSE_as_fiml <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_as', names30) & 
                       grepl('FIML', names30)])
colnames(RMSE_as_fiml) <- sample_sizes
rownames(RMSE_as_fiml) <- pars_30['a', ]

RMSE_as_dwls <- t(res30[,grepl('RMSE\\.', names30) & grepl('\\_as', names30) & 
                       grepl('DWLS', names30)])
colnames(RMSE_as_dwls) <- sample_sizes
rownames(RMSE_as_dwls) <- pars_30['a', ]

(out <- list(FIML=RMSE_as_fiml, DWLS=RMSE_as_dwls))
sapply(out, colMeans)
```

Again, the estimators appeared to recover the parameters with similar precision and bias. However, the
effect of parameter size now is more evident in the FIML estimator. Larger slopes indeed cause more progressively
bias and larger RMSE values compared to the DWLS estimator. In general, larger slopes when using FIML estimation
will be under-estimated, and therefore have larger RMSEs than the DWLS approach.

```{r}
library(ggplot2)
plt <- data.frame(pars = c(pars_30['a', ], pars_30['a', ]), 
                  RMSE = c(RMSE_as_fiml[,'1000'], RMSE_as_dwls[,'1000']),
                  bias = c(bias_as_fiml[,'1000'], bias_as_dwls[,'1000']),
                  estimator = rep(c('FIML', 'DWLS'), each = 30))
ggplot(plt, aes(pars, bias, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by bias for FIML and DWLS estimators')
ggplot(plt, aes(pars, RMSE, colour=estimator)) + geom_point(size=2) + facet_wrap(~estimator) + 
    ggtitle('slope sizes by RMSE for FIML and DWLS estimators')
```

```{r include=FALSE}
system('rm *.rds')
```

