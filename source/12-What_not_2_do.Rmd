%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
cache_ <- TRUE
```


# Comparison of For-Loop and SimDesign code

This is an example taken from the tutorial article: "Conducting Simulation Studies in the
R Programming Environment" in *Tutorials in Quantitative Methods for Psychology*
(2013, Vol. 9(2), p. 43-60) by Kevin A. Hallgren. While the majority of the article seems to be good at explaining the mechanics of performing Monte Carlo simulations in R, the implementation unfortunately used the `for` loop strategy, and demonstrates very nicely why `for` loops make coding simulations very difficult to read, modify, and debug. 

The following is the code from Appendix A regarding how Sobel's test for mediation performs. However, to save space the comments have been removed. As well, the original code itself actually contained typographical errors (a phenomenon Sigal and Chalmers, 2016, argued happens quite often with the `for`-loop strategy), which I have commented on below. For further details about the simulation, see the original article. 

# Simulation Using For-Loops

```{r eval=TRUE, cache=cache_}
N_list = c(100, 300)
a_list = c(-.3, 0, .3)
b_list = c(-.3, 0, .3)
cp_list = c(-.2, 0, .2) 
reps = 1000

set.seed(192)

sobel_test <- function(X, M, Y){
    M_X = lm(M ~ X)
    Y_XM = lm(Y ~ X + M)
    a = coefficients(M_X)[2]
    b = coefficients(Y_XM)[3]
    stdera = summary(M_X)$coefficients[2,2] 
    stderb = summary(Y_XM)$coefficients[3,2]
    sobelz = a*b / sqrt(b^2 * stdera^2 + a^2 * stderb^2) 
    return(sobelz)
}

# the following line was added to determine the computation time
time0 <- Sys.time()

d = NULL
for (N in N_list){
    for (a in a_list){ #this line was fixed; originally was "for (a in 1:a_list){"
        for (b in b_list){
            for (cp in cp_list){
                for (i in 1:reps){
                    X = rnorm(N, 0, 1)
                    M = a*X + rnorm(N, 0, 1)
                    Y = cp*X + b*M + rnorm(N, 0, 1)
                    d = rbind(d, c(i, a, b, cp, N, 1,
                                   sobel_test(X, M, Y)))
                    d = rbind(d, c(i, a, b, cp, N, 2,
                                   sobel_test(X, Y, M)))
                }
            }
        }
    }
}
colnames(d) = c("iteration", "a", "b", "cp", "N", "model", "Sobel_z")
d = data.frame(d)

# the following line was added to determine the computation time
time1 <- Sys.time()
time1 - time0
head(d)
```

# Simulation Using SimDesign

Compare the previous `for`-loop approach with `SimDesign`. Personally, I find it more telling what the $p$-values are doing instead of how the $z$-values are behaving, so the Sobel test function was modified to return both the $z$ and $p$-values.

```{r cache=cache_}
#modified to return z and p values
sobel_test_p <- function(X, M, Y){
    M_X = lm(M ~ X)
    Y_XM = lm(Y ~ X + M)
    a = coefficients(M_X)[2]
    b = coefficients(Y_XM)[3]
    stdera = summary(M_X)$coefficients[2,2] 
    stderb = summary(Y_XM)$coefficients[3,2]
    sobelz = unname(a*b / sqrt(b^2 * stdera^2 + a^2 * stderb^2))
    ret <- c(sobelz=sobelz, p=pnorm(abs(sobelz), lower.tail = FALSE)*2)
    ret
}

#-------------------------------------------------------------------

library(SimDesign)

Design <- createDesign(N = c(100, 300),
                       a = c(-.3, 0, .3),
                       b = c(-.3, 0, .3),
                       cp = c(-.2, 0, .2))

#-------------------------------------------------------------------

Generate <- function(condition, fixed_objects = NULL) {
    Attach(condition)
    X <- rnorm(N, 0, 1)
    M <- a*X + rnorm(N, 0, 1)
    Y <- cp*X + b*M + rnorm(N, 0, 1)
    dat <- data.frame(X, M, Y)
    dat
}

Analyse <- function(condition, dat, fixed_objects = NULL) {
	Attach(dat)
    XMY <- sobel_test_p(X, M, Y)
    XYM <- sobel_test_p(X, Y, M)
    ret <- c(XMY=XMY, XYM=XYM)
    ret
}

Summarise <- function(condition, results, fixed_objects = NULL) {
    ret <- EDR(results[,c('XMY.p', 'XYM.p')], alpha = .05)
    ret
}

#-------------------------------------------------------------------


# large results objects are saved to the hard drive
results <- runSimulation(design=Design, replications=1000, save_results=TRUE,
    generate=Generate, analyse=Analyse, summarise=Summarise, debug='none')
```

```{r}
print(results)
```

To see all the analyis results from `Design[1,]`, for example, inspect the object returned from `SimResults()`.

```{r eval=FALSE}
row1 <- SimResults(results, 1)
```

# Comparison

There are a few interesting properties to note about this comparison example. First, `SimDesign` is faster than the `for`-loop strategy, despite the fact it is technically doing more computations in the new Sobel test function and summarise steps. This is without running the `SimDesign` simulation code in parallel too, which would decrease the simulations times by a factor proportional to the number of cores available (and, unlike the `for`-loop code, only requires adding a `parallel = TRUE` flag to `runSimulation()`). Furthermore, the current state of the `for` loop code unfortunately cannot be used with parallel computations without a large amount of re-writing, because the object `d` is updated across each replications (therefore, the objects between each replication condition are not strictly independent).

Finally, the code itself is much less error prone and is much safer in case errors/warnings arise, the generation/analysis components are isolated better, the output is saved to external `.rds` files in order to inspect the results at a later time (thereby being more memory efficient), the code-base and results are generally more readable, ..., the list goes on. All this *despite the fact that the working code is nearly identical*. 