%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
```

# Simulation: Type I error rates and Power rates

Determine the Type I error and Power rates for a t-test when different groups sizes are used. The independent t-test has three main assumptions:

- independence of observations,
- normality of residuals (equivalent to normality of the data within each group), and
- homogeneity of variance.

The Welch test, on the other hand, only requires the first two assumptions. In the simulation below, we will also explore how robust the independent t-test is to this violation, and compare how well the Welch test behaves under the same conditions. 

### Define conditions

First, define the condition combinations that should be investigated. In this case we know that the size and
ratio of the sample sizes within each group, as well as the respective group standard deviations, will
influence the detection rates of the independent t-test. Here we study define three different sample sizes and
standard deviations, and investigate their effect when completely crossed.

```{r conditions}
library(SimDesign)
# SimFunctions()

Design <- expand.grid(sample_size = c(30, 60, 90, 120),
                      group_size_ratio = c(1/2, 1, 2),
                      standard_deviation_ratio = c(1, 4, 8),
                      mean_difference = c(0, 0.5))
dim(Design)
head(Design, 3)
tail(Design, 3)
```

Each row in `Design` represents a unique condition to be studied in the simulation. In this case, the first condition to be studied comes from row 1, where the total sample size is $N=30$ however the ratio of sample sizes within each group is different. In this case, the second group is half (0.5) the size of the first group, therefore $N1 = 20$ and $N2 = 10$. The standard deviation ratio is in this case 1 (i.e., they are exactly equal), and there is no mean difference between the groups. Analyzing rows with no mean difference will give us an idea of the empirical Type I error rates. 

The last condition on row 72 indicates that the second group has a sample size twice as large as the first,
and here $N1 = 40$ and $N2 = 80$. As well, group 2 has a standard deviation which is 8 times larger than the
first group, and overall there is a mean difference of 0.5. Truth be told, however, the interpretations 
could be switched around as it really depends on how the `Generate()` function utilizes these inputs
(the interpretation could have been that group 1 was *smaller* when `group_size_ratio == .5` rather than larger). 

### Define the functions

Begin by defining a data generation function for returning a `data.frame` with two columns: the dependent 
variable, and a grouping variable (a `factor` or otherwise).

```{r}
Generate <- function(condition, fixed_objects = NULL){
    N <- condition$sample_size
    grs <- condition$group_size_ratio
    sd <- condition$standard_deviation_ratio
    md <- condition$mean_difference
    if(grs < 1){
        N2 <- N / (1/grs + 1)
        N1 <- N - N2
    } else {
        N1 <- N / (grs + 1)
        N2 <- N - N1
    }
    group1 <- rnorm(N1)
    group2 <- rnorm(N2, mean=md, sd=sd)
    dat <- data.frame(group = c(rep('g1', N1), rep('g2', N2)), DV = c(group1, group2))
    dat
}
```

The `Generate()` function defined above first extracts the elements from `condition` for convenience,
generates data for two groups using random normal draws, and returns a data.frame object 
containing the dependent variable and grouping information. 

```{r}
Analyse <- function(condition, dat, fixed_objects = NULL, parameters = NULL){
    welch <- t.test(DV ~ group, dat)
    ind <- t.test(DV ~ group, dat, var.equal=TRUE)
    ret <- c(welch=welch$p.value, independent=ind$p.value)
    ret
}
```

The `Analyse()` definition performs a Welch and independent t-test on the data generated from `Generate()`, checks whether any errors occurred during the calls to the estimation functions, and extracts and returns the respective p-values for each statistical analysis.

Finally, after the number of replications defined in `replications` has been completed a matrix (or list, if the output of `Analyse()` were a list instead of a vector) the overall results should be summerised into suitable meta-descriptions of the behavior. This is accomplished with the `Summarise()` function.

```{r}
Summarise <- function(condition, results, fixed_objects = NULL, parameters_list = NULL){
    alpha.05 <- EDR(results, alpha = .05)
    ret <- c(alpha.05=alpha.05)
    ret
}
```

In the above function, the `results` input is a matrix of size `replications` by 2 containing the respective 
p-values returned from `Analyse()` across the desired number of replications. It is here where empirical 
detection rates can be determined, and a convenience function `EDR()` is available for just this purpose. Each column in the supplied matrix of p-values is checked to see whether it is less that $\alpha = .05$, and the
average number of times this occurs is returned as a vector of length 2. Names are preserved in the `EDR()` 
function, therefore it is clear which element represents the Welch and Independent t-test rates.

### Run the simulation

In this case a few extra arguments are passed to `runSimulation()`. To help speed things up, the use 
of parallel architecture is used by passing `parallel = TRUE`, and messages printed to the console are 
disabled by passing `verbose = FALSE`. 

```{r include=FALSE}
set.seed(1234)
```


```{r cache=TRUE}
results <- runSimulation(design = Design, replications = 1000, verbose = FALSE, parallel = TRUE,
                       generate = Generate, analyse = Analyse, summarise = Summarise)
```

### Analyse the results

Usually it is a good idea to split up analyses into Type I error and power data, and to make 
design conditions into `factor`s if necessary. More specifically, Power rates really should only be interpreted when the Type I errors are well behaved (liberal detection rates naturally will result in more powerful tests, while conservative detection rates will induce lower power).

```{r}
head(results)
results$REPLICATIONS <- results$SIM_TIME <- NULL
TypeI <- subset(results, mean_difference == 0)
Power <- subset(results, mean_difference != 0)
```

Lets start off by looking at some descriptive tables, and performing simple ANOVA analyses with effect sizes
to determine where the differences generally occurred.

```{r}
library(reshape2)
mlt <- melt(TypeI, id.vars = colnames(TypeI)[1:4])
out <- dcast(mlt, group_size_ratio + standard_deviation_ratio + sample_size ~ variable)
colnames(out) <- c('Group Ratio', 'SD Ratio', 'N', 'Welch', 'Independent')
emph <- suppressWarnings(which(out > .075 | out < .025, arr.ind = TRUE))
pander::pander(out, round=2, emphasize.strong.cells = emph)

out2 <- dcast(mlt, group_size_ratio + standard_deviation_ratio ~ variable, fun.aggregate = mean)
colnames(out2) <- c('Group Ratio', 'SD Ratio', 'Welch', 'Independent')
emph2 <- suppressWarnings(which(out2 > .075 | out2 < .025, arr.ind = TRUE))
pander::pander(out2, round=2, emphasize.strong.cells = emph2)
```

The descriptive rates seem to be very favorable for the Welch test in that they are all close to the nominal
detection rate. However, the independent t-test appears to have combinations where it is either liberal or 
conservative. Though we could simply inspect the table to see why/where this occurred, sometimes it is easier
to use ANOVAs to help pinpoint where the sources of variation occurred. 

The one catch with ANOVAs is that there must be within-group variability, therefore something in our design must be marginalized. In this case, we will marginalize sample size because Type I error rates really shouldn't be influenced by $N$. After inspecting the previous table though, the assumption that $N$ does not affect the results is probably a good one.

```{r}
SimAnova(alpha.05.independent ~ sample_size, TypeI)
SimAnova(alpha.05.independent ~ group_size_ratio * standard_deviation_ratio, TypeI)
```

After examining the effect sizes for the independent t-test, there appears to be an interaction effect between `group_size_ratio` and `standard_deviation_ratio`. To help visualize this effect we create box-plots while continuing to marginlize over the sample size (otherwise, we would have to use point estimates for scatter plots). 

```{r}
SimAnova(alpha.05.welch ~ group_size_ratio * standard_deviation_ratio, TypeI)
```

```{r}
library(ggplot2)

#welch
gg <- ggplot(TypeI, aes(group_size_ratio, alpha.05.welch, fill = standard_deviation_ratio))
gg + geom_boxplot() + facet_wrap(~standard_deviation_ratio) + ylim(c(0,0.2)) + 
    geom_hline(yintercept = .075, colour = 'red') + geom_hline(yintercept = .025, colour = 'red')

#independent
gg <- ggplot(TypeI, aes(group_size_ratio, alpha.05.independent, fill = standard_deviation_ratio))
gg + geom_boxplot() + facet_wrap(~standard_deviation_ratio) + ylim(c(0,0.2)) + 
    geom_hline(yintercept = .075, colour = 'red') + geom_hline(yintercept = .025, colour = 'red')
```

What is immediately obvious from these figures is that the Welch test performs fairly well across all conditions for controlling Type I error rates, while the independent t-test is heavily influenced by different combinations. When the variances were equal (left facet plot), the independent t-test appeared to perform well, and this should not be surprising because all the assumptions were met. As well, when the sample sizes were equal the test seemed to perform well across the board.

However, when the second group contained a larger variance AND a larger sample size than the first group the detection rates became more conservative. The converse was true when larger variances were paired with smaller sample sizes compared to the first group, where the detection rates became progressively more liberal as both the ratio of sample sizes increase and the ratio of standard deviations increased.

