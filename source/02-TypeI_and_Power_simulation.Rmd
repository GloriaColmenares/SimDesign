%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
date: "September 22, 2015"
output:
  html_document:
    number_sections: yes
    toc: yes
---

```{r include=FALSE}
options(digits = 3)
```

# Simulation: Type I error rates and Power rates

Determine the Type I error and Power rates for a t-test when different groups sizes are used and when
the assumption of homogeneity of variance is violated.

### Define conditions

First, define the condition combinations that should be investigated. In this case we know that the size and
ratio of the sample sizes within each group, as well as the respective group standard deviations, will
influence the detection rates of the independent t-test. Here we study define three different sample sizes and
standard deviations, and investigate their effect when completely crossed.

```{r conditions}
sample_sizes <- c(10, 20, 50, 100)
standard_deviation_ratios <- c(1, 4, 8)
mean_difference <- c(0, 0.5)

Design <- expand.grid(sample_size_group1=sample_sizes,
                      sample_size_group2=sample_sizes,
                      standard_deviation_ratio=standard_deviation_ratios,
                      mean_difference=mean_difference)
dim(Design)
head(Design, 3)
tail(Design, 3)
```

Each row in `Design` represents a unique condition to be studied in the simulation. In this case, the first condition to be studied comes from row 1, where both groups have a sample size of 10 and the ratio of the standard deviations are 1 (i.e., they are exactly equal) and there is no mean difference between the groups. Analysing rows with no mean difference will give us an idea of the Type I error rates. The last condition on row 96 indicates that both groups have sample sizes of 100 but that one of the standard deviations is 8 times larger than the other and a mean difference of 0.5. Which group is larger in both the standard deviation and mean will be defined in the `Generate()` function. 

### Define the functions

Again, it's usually good to start by editing template functions from `SimDesign_functions()` to ensure that
all arguments are defined. Below this is simply commented out.

```{r}
library(SimDesign)
# SimDesign_functions()
```

Begin by defining a data generation function for returning a `data.frame` with two columns: the dependent 
variable, and a grouping variable (a `factor` or otherwise).

```{r}
Generate <- function(condition){
 
    N1 <- condition$sample_size_group1
    N2 <- condition$sample_size_group2
    sd <- condition$standard_deviation_ratio
    md <- condition$mean_difference
    
    group1 <- rnorm(N1)
    group2 <- rnorm(N2, mean=md, sd=sd)
    dat <- data.frame(group = c(rep('g1', N1), rep('g2', N2)), DV = c(group1, group2))
    
    return(dat)
}
```

The `Generate()` function defined above first extracts the elements from `condition` for convenience,
generates data for two groups using random normal draws, and returns a data.frame object 
containing the dependent variable and grouping information. 

```{r}
Analyse <- function(condition, dat, parameters = NULL){
    
    require(stats)

    welch <- try(t.test(DV ~ group, dat), silent=TRUE)
    ind <- try(t.test(DV ~ group, dat, var.equal=TRUE), silent=TRUE)
    check_error(welch, ind)

    ret <- c(welch=welch$p.value, independent=ind$p.value)
    return(ret)
}
```

The `Analyse()` definition performs a Welch and independent t-test on the data generated from `Generate()`, checks whether any errors occurred during the calls to the estimation functions, and extracts and returns the respective p-values for each statistical analysis.

Finally, after the number of replications defined in `replications` has been completed a matrix (or list, if the output of `Analyse()` were a list instead of a vector) the overall results should be summerised into suitable meta-descriptions of the behavior. This is accomplished with the `Summarise()` function.

```{r}
Summarise <- function(condition, results, parameters_list = NULL){
    
    nms <- c('welch', 'independent')
    alpha.05 <- EDR(results[,nms], alpha = .05)
    
    ret <- c(alpha.05=alpha.05)
    return(ret)
}
```

In the above function, the `results` input is a matrix of size `replications` by 2 containing the respective 
p-values returned from `Analyse()` across the desired number of replications. It is here where empirical 
detection rates can be determined, and a convenience function `EDR()` is available for just this purpose. Each column in the supplied matrix of p-values is checked to see whether it is less that $\alpha = .05$, and the
average number of times this occurs is returned as a vector of length 2. Names are preserved in the `EDR()` 
function, therefore it is clear which element represents the Welch and Independent t-test rates.

### Run the simulation

In this case a few extra arguments are passed to `runSimulation()`. To help speed things up, the use 
of parallel architecture is used by passing `parallel = TRUE`, and messages printed to the console are 
disabled by passing `verbose = FALSE`. 

```{r cache=TRUE}
Final <- runSimulation(design = Design, replications = 1000, verbose = FALSE, parallel = TRUE,
                       generate = Generate, analyse = Analyse, summarise = Summarise)
```

### Analyse the results

Usually it is a good idea to split up analyses into Type I error and power data, and to make 
design conditions into `factor`s if necessary. More specifically, Power rates really should only be interpreted when the Type I errors are well behaved (liberal detection rates naturally will result in more powerful tests, while conservative detection rates will induce lower power).

```{r}
for(i in 1:4) Final[,i] <- factor(Final[,i])
TypeI <- subset(Final, mean_difference == 0)
Power <- subset(Final, mean_difference != 0)
```

Rather than looking at the numbers, lets jump straight to graphical representations of the effects. 
Because there is only three facets to look at, plotting is not an issue. With more than three factors,
however, the use of ANOVA's with a number of interaction terms and effect sizes can be very helpful to detect
the important effects present in the simulation.

```{r}
library(ggplot2)

#welch
gg <- ggplot(TypeI, aes(sample_size_group1, alpha.05.welch, colour = sample_size_group2))
gg + geom_point(size=3) + facet_wrap(~standard_deviation_ratio) + ylim(c(0,1)) + 
    geom_hline(yintercept = .075, colour = 'red') + geom_hline(yintercept = .025, colour = 'red')

#independent
gg <- ggplot(TypeI, aes(sample_size_group1, alpha.05.independent, colour = sample_size_group2))
gg + geom_point(size=3) + facet_wrap(~standard_deviation_ratio) + ylim(c(0,1)) + 
    geom_hline(yintercept = .075, colour = 'red') + geom_hline(yintercept = .025, colour = 'red')
```

What is immediately obvious from these figures is that the Welch test performs fairly well across all conditions for controlling Type I error rates, while the independent t-test is heavily influenced by different combinations. When the variances were equal (left facet plot), the test appeared to perform well, and should not be surprising because all the assumptions were met. As well, when the sample sizes were equal the test seemed to perform well across the board.

However, when the second group contained a larger variance AND a larger sample size than the first group the detection rates became more conservative. The converse was true when larger variances were paired with smaller sample sizes compared to the first group, where the detection rates became progressively more liberal as both the ratio of sample sizes increase and the ratio of standard deviations increased.

